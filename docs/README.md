# DataFlow.NET Framework - Technical Documentation

## Table of Contents

1. [Framework Overview](#framework-overview)
2. [Unified Processing Architecture](#unified-processing-architecture)
3. [Architecture](#architecture)
4. [Quick Start Guide](#quick-start-guide)
5. [Layer Documentation](#layer-documentation)
   - [DataFlow.Data Layer](#DataFlow-data-layer)
   - [DataFlow.Extensions Layer](#DataFlow-extensions-layer)
   - [DataFlow.Framework Layer](#DataFlow-framework-layer)
6. [Stream Processing Deep Dive](#stream-processing-deep-dive)
7. [API Reference](#api-reference)
8. [Advanced Topics](#advanced-topics)
9. [Best Practices](#best-practices)
10. [Performance Guide](#performance-guide)

## 1. Framework Overview

DataFlow.NET is an open-source framework that **unifies batch and streaming data processing** in C#. The framework's core innovation is providing **identical fluent and composable syntax** for processing static files and real-time data streams, enabling developers to write processing logic once and deploy it across different data paradigms without code changes.

Beyond transformation, **DataFlow.NET provides production-grade data ingestion** with sophisticated async readers featuring enterprise-level error handling, security hardening, and observability.

### 1.1 Key Features

- **üîÑ Unified Processing Model**: Identical APIs for `IEnumerable<T>` and `IAsyncEnumerable<T>`
- **‚ö° Stream-First Architecture**: Built on async enumeration with sync compatibility
- **üéØ Intelligent Category Processing**: Sophisticated "Supra Category" pattern for selective data handling
- **üìä Multi-Source Stream Merging**: `AsyncEnumerable<T>` for unifying multiple async sources
- **üöÄ Lazy Evaluation**: Process data streams efficiently with minimal memory footprint
- **üí° Fluent API**: Chain operations using intuitive, readable syntax
- **üìñ Production-Grade Readers**: CSV, JSON, YAML with error management, metrics, security hardening, schema inference, guard rails, and progress reporting ([full documentation](DataFlow-Data-Reading-Infrastructure.md))
- **üìÅ Multi-format Support**: Handle CSV, text, JSON, YAML, and custom data formats seamlessly
- **üîç Regular Expression Integration**: Simplified regex patterns with powerful matching capabilities
- **üõ°Ô∏è Type Safety**: Strong typing with compile-time validation
- **‚ö° Performance Optimized**: Efficient algorithms for large dataset processing

### 1.2 Design Philosophy

DataFlow.NET makes **streaming as simple as working with lists**. The framework uses **pull-based, non-buffering streams** where data flows only when consumed. All complexity‚Äîbackpressure, buffering, error handling‚Äîis managed at the **data entry boundary** through reader configuration. Once data enters your pipeline, processing is just LINQ-compatible operations on `IAsyncEnumerable<T>`. Configure the hard parts once at the readers, then enjoy simple, readable transformation logic.

## 2. Unified Processing Architecture

### 2.1 Configuration-Driven Transformation Trees

DataFlow.NET introduces a revolutionary **Cases/SelectCase/ForEachCase pattern** that lets you configure complex, multi-branch transformation trees **declaratively**. Despite defining multiple transformation paths upfront, the framework executes them **lazily and efficiently**‚Äîeach item flows through the pipeline **exactly once**, with **zero buffering** and **minimal memory footprint**.

```csharp
// Configure a complete transformation tree ONCE
await dataSource
    .Cases(
        data => data.Type == "Customer",
        data => data.Type == "Order", 
        data => data.Type == "Product"
    )
    .SelectCase(
        customer => EnrichCustomer(customer),      // Branch 1: Transform
        order => CalculateOrderTotal(order),       // Branch 2: Transform
        product => NormalizeProduct(product),      // Branch 3: Transform
        unknown => LogUnknownType(unknown)         // Supra: Catch-all
    )
    .ForEachCase(
        customer => await customerDB.SaveAsync(customer),   // Branch 1: Side-effect
        order => await orderDB.SaveAsync(order),            // Branch 2: Side-effect
        product => await productDB.SaveAsync(product),      // Branch 3: Side-effect
        unknown => await errorLogger.LogAsync(unknown)      // Supra: Error handling
    )
    .AllCases()
    .WriteCsv("processed_output.csv");

// ‚úÖ Tree configured once, executed lazily
// ‚úÖ Each item processed exactly once
// ‚úÖ No intermediate collections
// ‚úÖ Constant memory usage
```

**Key Benefits:**

- **üéØ Declarative Configuration**: Define all transformation branches upfront in a readable, maintainable way
- **‚ö° Single-Pass Execution**: Despite multiple branches, each item flows through the pipeline exactly once
- **üíæ Memory Efficient**: Lazy evaluation means zero buffering‚Äîprocess gigabytes with constant memory
- **üîß Developer-Friendly**: Simple, configuration-like syntax that reads like a decision tree

---

### 2.2 Write Once, Process Anywhere

The framework's **unified processing model** delivers on a revolutionary promise: **write your processing logic once, deploy it across batch and streaming paradigms without code changes**.

#### Identical Syntax Across Paradigms

```csharp
// Define processing logic ONCE
public static async Task ProcessBusinessLogic<T>(T dataSource) 
    where T : IAsyncEnumerable<Transaction>
{
    await dataSource
        .Cases(
            tx => tx.Amount > 10000,
            tx => tx.IsFlagged,
            tx => tx.Country != "US"
        )
        .SelectCase(
            highValue => ProcessHighValue(highValue),
            suspicious => ProcessSuspicious(suspicious),
            international => ProcessInternational(international),
            standard => ProcessStandard(standard)
        )
        .ForEachCase(
            highValue => await complianceDB.SaveAsync(highValue),
            suspicious => await fraudDB.SaveAsync(suspicious),
            international => await forexDB.SaveAsync(international),
            standard => await standardDB.SaveAsync(standard)
        )
        .AllCases()
        .WriteCsv("processed_transactions.csv");
}

// BATCH: Historical file processing
await ProcessBusinessLogic(
    Read.Csv<Transaction>("historical_data.csv")
);

// STREAMING: Real-time event processing (IDENTICAL CODE!)
await ProcessBusinessLogic(liveTransactionStream);
```

#### Zero-Cost Migration Path

```csharp
// DEVELOPMENT: Start with in-memory data
var testData = new[] { 
    new Order { Id = 1, IsUrgent = true },
    new Order { Id = 2, IsUrgent = false }
}.Async(); // Async is a DataFlow.Net Extension to expose a Stream like interface ( IAsycEnumerable) for In-memory data:(Type=Table, List Or any other IEnumerable  )

// VALIDATION: Test with static files
var devPipeline = Read.Csv<Order>("test_orders.csv")
    .Cases(IsUrgent, IsInternational, IsHighValue)
    .SelectCase(
        urgent => ProcessUrgent(urgent),
        international => ProcessInternational(international),
        highValue => ProcessHighValue(highValue),
        standard => ProcessStandard(standard)
    )
    .AllCases();

await devPipeline.WriteJson("dev_results.json");

// PRODUCTION: Deploy to live streams (ZERO CODE CHANGES!)
var prodPipeline = liveOrderStream
    .Cases(IsUrgent, IsInternational, IsHighValue)     // Same predicates
    .SelectCase(
        urgent => ProcessUrgent(urgent),                // Same transforms
        international => ProcessInternational(international),
        highValue => ProcessHighValue(highValue),
        standard => ProcessStandard(standard)
    )
    .AllCases();

await prodPipeline.WriteJson("prod_results.json");
```

**Migration Benefits:**

- ‚úÖ **Develop with in-memory tables**: Test and debug directly in your IDE
- ‚úÖ **Validate performance with files**: Benchmark with realistic datasets using CSV/JSON files
- ‚úÖ **Deploy to streams**: Switch to Kafka/EventHub/SignalR without refactoring
- ‚úÖ **Zero code changes**: Same predicates, same transformations, same side-effects
- ‚úÖ **Risk-free evolution**: Validate logic in batch before streaming deployment

---

### 2.3 Forget Data Parsing Complexity

**Format changes? Change one word.** DataFlow.NET eliminates parsing boilerplate‚Äîno manual stream handling, no scattered try-catch blocks, no custom error logging.

```csharp
// JSON today
var data = Read.Json<Order>("orders.json");

// YAML tomorrow (same business logic)
var data = Read.Yaml<Order>("orders.yaml");

// CSV next week (still same logic)
var data = Read.Csv<Order>("orders.csv");

// Processing stays identical
await data
    .Cases(IsUrgent, IsStandard)
    .SelectCase(urgent => ProcessUrgent(urgent), standard => ProcessStandard(standard))
    .AllCases()
    .WriteJson("output.json");
```

**Error handling? Configure once at the boundary.**

```csharp
var options = new CsvReadOptions {
    ErrorAction = ReaderErrorAction.Skip,
    ErrorSink = new JsonLinesFileErrorSink("errors.ndjson"),
    Progress = new Progress<ReaderProgress>(p => Console.WriteLine($"{p.RecordsRead} rows"))
};

await Read.Csv<Order>("orders.csv", options)
    .Select(order => EnrichOrder(order))      // Clean transformation
    .Where(order => order.IsValid)            // No try-catch needed
    .Cases(IsUrgent, IsStandard)              // Pure business logic
    .ForEachCase(
        urgent => await ProcessUrgent(urgent),
        standard => await ProcessStandard(standard)
    )
    .AllCases()
    .WriteCsv("processed.csv");

// Check metrics after
Console.WriteLine($"Processed: {options.Metrics.RecordsEmitted}, Errors: {options.Metrics.ErrorCount}");
```

**Stop writing parsing code. Start writing business logic.**

### 2.4 And more : Multi-Source Stream? manage it as one!

The `AsyncEnumerable<T>` class provides **declarative configuration** for merging multiple real-time data sources into a single unified stream. Configure once, process with familiar syntax.

```csharp
// Configure unified stream (declarative, simple)
var unifiedLogs = new AsyncEnumerable<LogEntry>(new UnifyOptions
{
    Fairness = UnifyFairness.RoundRobin,      // Fair scheduling across sources
    ErrorMode = UnifyErrorMode.ContinueOnError // Resilient to source failures
})
.Unify(webServerLogs,  name: "web",  predicate: log => log.Level >= LogLevel.Info)
.Unify(databaseLogs,   name: "db",   predicate: log => log.Level >= LogLevel.Warning)
.Unify(authServiceLogs, name: "auth", predicate: log => log.Level >= LogLevel.Error);

// Process with standard Cases pattern (identical to single-source)
await unifiedLogs
    .Cases(
        log => log.Level == LogLevel.Error,
        log => log.Level == LogLevel.Warning,
        log => log.Service == "Database"
    )
    .SelectCase(
        error => $"CRITICAL: {error.Service} - {error.Message}",
        warning => $"WARN: {warning.Service} - {warning.Message}",
        dbLog => $"DB: {dbLog.Message}",
        other => $"INFO: {other.Message}"
    )
    .ForEachCase(
        error => await alertSystem.SendCriticalAsync(error),
        warning => await alertSystem.SendWarningAsync(warning),
        dbLog => await dbMonitor.RecordAsync(dbLog),
        other => await generalLogger.LogAsync(other)
    )
    .AllCases()
    .WriteText("unified_logs.txt");
```

**Key Features:**

- **üîå Declarative Source Registration**: `Unify()` calls configure sources with optional per-source filtering
- **‚öñÔ∏è Fairness Policies**: `FirstAvailable` (performance) or `RoundRobin` (fairness)
- **üõ°Ô∏è Error Resilience**: `FailFast` (strict) or `ContinueOnError` (drop failing sources)
- **üîÑ Zero Buffering**: Pull-based streaming with no built-in buffering (opt-in via extensions)
- **üìä Identical Processing**: Once unified, process with same Cases/SelectCase/ForEachCase patterns

## 3. Architecture

The DataFlow.NET framework follows a **three-layer architecture** optimized for unified batch and streaming processing:

### 3.1 DataFlow.Data

**Unified Data Access Layer**

- File I/O operations (Read, Write) with async support
- Data format handling (CSV, Text, JSON, YAML) for both files and streams
- Data mapping and transformation utilities
- **Stream-aware readers** that work with both files and live data sources

### 3.2 DataFlow.Extensions

**Unified Extension Methods Layer**

- **Dual IEnumerable/IAsyncEnumerable extensions** for data manipulation
- String processing utilities with async support
- File system extensions for streaming scenarios
- Type conversion and parsing extensions
- **Cases/SelectCase/ForEachCase pattern** for both sync and async

### 3.3 DataFlow.Framework

**Stream Processing Infrastructure Layer**

- **AsyncEnumerable<T>** for multi-source stream union/merging
- Channel-based async communication
- Regular expression utilities with stream support
- Syntax parsing capabilities

## 4. Quick Start Guide

### 4.1 Installation

```bash
# Add reference to your project
dotnet add reference DataFlow.NET

# Or via NuGet (coming soon)
dotnet add package DataFlow.NET
```

### 4.2 Basic Unified Processing Example

```csharp
using DataFlow.Data;
using DataFlow.Extensions;
using DataFlow.Framework;

// Define your data structure
public struct LogEntry
{
    public DateTime Timestamp;
    public string Level;
    public string Message;
    public string Service;
}

// BATCH PROCESSING: Read from file
var batchResults = Read.Csv<LogEntry>("historical_logs.csv", ",")
    .Cases(
        log => log.Level == "ERROR",
        log => log.Level == "WARNING"
    )
    .SelectCase(
        error => $"üö® {error.Service}: {error.Message}",
        warning => $"‚ö†Ô∏è {warning.Service}: {warning.Message}",
        info => $"‚ÑπÔ∏è {info.Service}: {info.Message}"
    )
    .AllCases()
    .WriteText("processed_batch.log");

// STREAMING PROCESSING: Same logic, different sources via AsyncEnumerable merger
var liveLogStream = new AsyncEnumerable<LogEntry>()
    .Unify(webServerLogs, "web")
    .Unify(databaseLogs, "db")
    .Unify(authServiceLogs, "auth");
 

var streamResults = await liveLogStream
    .Cases(
        log => log.Level == "ERROR",    // Same predicates
        log => log.Level == "WARNING"   // Same categorization
    )
    .SelectCase(
        error => $"üö® {error.Service}: {error.Message}",     // Same transformations
        warning => $"‚ö†Ô∏è {warning.Service}: {warning.Message}",
        info => $"‚ÑπÔ∏è {info.Service}: {info.Message}"
    )
    .AllCases()                        // Same result extraction
    .WriteText("processed_stream.log");  // Async version for streaming
```

### 4.3 Advanced Stream Processing Example

```csharp
// Set up assumption: multiple async sources (IAsyncEnumerable<T>)
// IAsyncEnumerable<Order> GetLiveOrdersAsync();
// IAsyncEnumerable<InventoryUpdate> GetInventoryUpdatesAsync();

var orders = GetLiveOrdersAsync();
var inventory = GetInventoryUpdatesAsync();

// Merge homogeneous streams (per type) using AsyncEnumerable
var orderProcessor = new AsyncEnumerable<Order>()
    .Unify(orders, "Orders");
var inventoryProcessor = new AsyncEnumerable<InventoryUpdate>()
    .Unify(inventory, "Inventory");

// Process orders in real-time
var orderTask = orderProcessor
    .Cases(
        order => order.Total > 1000,        // High-value orders
        order => order.IsInternational,     // International orders
        order => order.Customer.IsVIP       // VIP customer orders
    )
    .SelectCase(
        highValue => ProcessHighValueOrder(highValue),
        international => ProcessInternationalOrder(international),
        vip => ProcessVIPOrder(vip),
        standard => ProcessStandardOrder(standard)
    )
    .ForEachCase(
        async highValue => await complianceSystem.ReviewAsync(highValue),
        async international => await currencyService.ProcessAsync(international),
        async vip => await vipService.PrioritizeAsync(vip),
        async standard => await standardQueue.EnqueueAsync(standard)
    )
    .AllCases()
    .WriteCsv("processed_orders.csv");

// Process inventory updates simultaneously
var inventoryTask = inventoryProcessor
    .Cases(
        update => update.NewQuantity == 0,   // Out of stock
        update => update.NewQuantity < 10    // Low stock
    )
    .SelectCase(
        outOfStock => CreateRestockOrder(outOfStock),
        lowStock => CreateLowStockAlert(lowStock),
        normal => LogInventoryChange(normal)
    )
    .ForEachCase(
        async restock => await purchasingSystem.OrderAsync(restock),
        async alert => await alertSystem.NotifyAsync(alert),
        async log => await auditLogger.LogAsync(log)
    )
    .AllCases()
    .WriteJson("inventory_updates.json");

// Run both processors concurrently
await Task.WhenAll(orderTask, inventoryTask);
```

## 5. Layers Description

### 5.1 DataFlow.Data Layer

*Note: Default method names are **ASYNCHRONOUS**. Synchronous variants use the `Sync` suffix.*

#### Read Class - Unified Data Reading

The `Read` class provides static methods for reading data from various sources with **lazy evaluation** and **stream compatibility**. It offers a unified API for handling different data formats like Text, CSV, JSON, and YAML, with robust error handling and configuration options.

For a comprehensive guide on advanced configuration, error handling strategies, format-specific options, and known limitations, please refer to the detailed documentation:

- **[Deep Dive: DataFlow.Data Reading Infrastructure](DataFlow-Data-Reading-Infrastructure.md)**

**Key Methods:**

```csharp
// Text file reading (works with both files and streams)
public static IEnumerable<string> TextSync(string path)
public static IEnumerable<string> TextSync(StreamReader file)
public static IAsyncEnumerable<string> Text(string path)
public static IAsyncEnumerable<string> Text(StreamReader file)

// CSV reading with type mapping
public static IEnumerable<T> CsvSync<T>(string path, string separator = ",")
public static IAsyncEnumerable<T> Csv<T>(string path, string separator = ",")

// JSON/YAML reading
public static IEnumerable<T> JsonSync<T>(string path)
public static IAsyncEnumerable<T> Json<T>(string path)

```

**Example:**

```csharp
// Batch reading
var lines = Read.TextSync("data.txt");
var records = Read.CsvSync<MyRecord>("data.csv", ";");

// Stream reading (async)
await foreach (var line in Read.Text("large_file.txt"))
{
    await ProcessLineAsync(line);
}

// Both work with the same processing pipeline!
var batchResult = Read.CsvSync<Order>("orders.csv").Cases(IsUrgent, IsStandard)
var streamResult = await Read.Csv<Order>("live_orders.csv").Cases(IsUrgent, IsStandard);
```

#### Writers Class - Unified Data Writing

Extension methods for writing data to various formats with **async support**.

**Key Methods:**

```csharp
// Synchronous writing
public static void WriteTextSync(this IEnumerable<string> lines, string path, CancellationToken ct = default);
public static void WriteCsvSync<T>(this IEnumerable<T> records, string path, bool withTitle = true, string separator = ",", CancellationToken ct = default);
public static void WriteJsonSync<T>(this IEnumerable<T> items, string path, CancellationToken ct = default);
public static void WriteYamlSync<T>(this IEnumerable<T> items, string path, CancellationToken ct = default);


// Asynchronous writing for streams
public static Task WriteText(this IEnumerable<string> lines, string path, CancellationToken ct = default);
public static Task WriteText(this IAsyncEnumerable<string> lines, string path), CancellationToken ct = default;

public static Task WriteCsv<T>(this IEnumerable<T> records, string path, bool withTitle = true, string separator = ",", CancellationToken ct = default);
public static Task WriteCsv<T>(this IAsyncEnumerable<T> records, string path, bool withTitle = true, string separator = ",", CancellationToken ct = default);

public static Task WriteJson<T>(this IEnumerable<T> items, string path, CancellationToken ct = default);
public static Task WriteJson<T>(this IAsyncEnumerable<T> items, string path, CancellationToken ct = default);

public static Task WriteYaml<T>(this IEnumerable<T> items, string path, bool writeEmptySequenceWhenNoItems = true, CancellationToken ct = default)
public static Task WriteYaml<T>(this IAsyncEnumerable<T> items, string path, bool writeEmptySequenceWhenNoItems = true, CancellationToken ct = default)

```

### 5.2 DataFlow.Extensions Layer

#### Unified IEnumerable/IAsyncEnumerable Extensions

**The core innovation**: Identical extension methods for both `IEnumerable<T>` and `IAsyncEnumerable<T>`.

**Core Processing Extensions:**

```csharp
// Categorization (works for both sync and async)
public static IEnumerable<(int category, T item)> Cases<T>(
    this IEnumerable<T> items, params Func<T, bool>[] filters)

public static IAsyncEnumerable<(int category, T item)> Cases<T>(
    this IAsyncEnumerable<T> items, params Func<T, bool>[] filters)

// Transformation per category
public static IEnumerable<(int category, T item, R newItem)> SelectCase<T, R>(
    this IEnumerable<(int category, T item)> items, params Func<T, R>[] selectors)

public static IAsyncEnumerable<(int category, T item, R newItem)> SelectCase<T, R>(
    this IAsyncEnumerable<(int category, T item)> items, params Func<T, R>[] selectors)

// Actions per category
public static IEnumerable<(int category, T item, R newItem)> ForEachCase<T, R>(
    this IEnumerable<(int category, T item, R newItem)> items, params Action<R>[] actions)

public static IAsyncEnumerable<(int category, T item, R newItem)> ForEachCase<T, R>(
    this IAsyncEnumerable<(int category, T item, R newItem)> items, params Func<R, Task>[] actions)

// Result extraction
public static IEnumerable<R> AllCases<T, R>(
    this IEnumerable<(int category, T item, R newItem)> items)

public static IAsyncEnumerable<R> AllCases<T, R>(
    this IAsyncEnumerable<(int category, T item, R newItem)> items)
```

**Utility Extensions:**

```csharp
// Conditional processing
public static IEnumerable<T> Until<T>(this IEnumerable<T> items, Func<T, bool> condition)
public static IAsyncEnumerable<T> Until<T>(this IAsyncEnumerable<T> items, Func<T, bool> condition)

// Debugging and monitoring
public static IEnumerable<T> Spy<T>(this IEnumerable<T> items, Action<T> action)
public static IAsyncEnumerable<T> Spy<T>(this IAsyncEnumerable<T> items, Func<T, Task> action)

// Aggregation
public static IEnumerable<T> Cumul<T>(this IEnumerable<T> items, Func<T, T, T> accumulator)
public static IAsyncEnumerable<T> Cumul<T>(this IAsyncEnumerable<T> items, Func<T, T, T> accumulator)
```

#### The Supra Category Pattern

The **Supra Category Pattern** is DataFlow.NET's signature feature for intelligent, selective data processing. This pattern automatically handles items that don't match any provided categorization criteria.

##### How It Works

When using `Cases()` to categorize data:

1. Items matching the first predicate get category `0`
2. Items matching the second predicate get category `1`
3. Items matching the nth predicate get category `n-1`
4. **Items not matching ANY predicate get category `n` (the "supra category")**

##### Selective Processing Philosophy

The supra category enables a **"selective processing"** approach:

- **Express Intent**: Provide selectors/actions only for categories you care about
- **Graceful Ignoring**: Missing selectors return `default(T)`, enabling natural filtering
- **Future-Proof**: New data patterns don't break existing processing pipelines
- **Performance Optimized**: Single-pass processing with minimal overhead

##### Batch Processing Example

```csharp
// Process only ERROR and WARNING logs, ignore everything else
var processedLogs = Read.Text("application.log")
    .Cases(
        line => line.Contains("ERROR"),   // Category 0
        line => line.Contains("WARNING")  // Category 1
        // DEBUG, INFO, TRACE, etc. -> Category 2 (supra category)
    )
    .SelectCase(
        error => $"üö® CRITICAL: {error}",     // Handle category 0
        warning => $"‚ö†Ô∏è WARNING: {warning}"   // Handle category 1
        // No selector for category 2 -> gets default(string) = null
    )
    .Where(result => result.newItem != null)  // Filter out ignored items
    .AllCases();
```

##### Streaming Processing Example

```csharp
// Same logic works for streaming data!
var processedStream = await liveLogStream
    .Cases(
        log => log.Level == LogLevel.Error,   // Category 0
        log => log.Level == LogLevel.Warning  // Category 1
        // Info, Debug, Trace -> Category 2 (supra category)
    )
    .SelectCase(
        error => new Alert { Level = AlertLevel.Critical, Message = error.Message },
        warning => new Alert { Level = AlertLevel.Warning, Message = warning.Message }
        // No selector for supra category -> gets default(Alert) = null
    )
    .Where(x => x.newItem != null)  // Filter out ignored categories
    .ForEachCase(
        critical => await alertSystem.SendImmediateAsync(critical),
        warning => await alertSystem.QueueAsync(warning)
    )
    .AllCases()
    .WriteJson("processed_alerts.json");
```

##### Advanced Multi-Category Stream Processing

```csharp
// Complex streaming scenario with multiple selective stages
var transactionAlerts = await liveTransactionStream
    .Cases(
        tx => tx.Amount > 10000,           // Category 0: High value
        tx => tx.IsFlagged,                // Category 1: Suspicious
        tx => tx.IsInternational,          // Category 2: International
        tx => tx.Customer.IsVIP            // Category 3: VIP customer
        // Regular transactions -> Category 4 (supra category)
    )
    .SelectCase(
        highValue => new ComplianceReview { Transaction = highValue, Priority = Priority.High },
        suspicious => new FraudInvestigation { Transaction = suspicious, Urgent = true },
        international => new CurrencyConversion { Transaction = international },
        vip => new VIPProcessing { Transaction = vip, FastTrack = true }
        // No selector for regular transactions -> they get default(object) = null
    )
    .Where(x => x.newItem != null)  // Remove regular transactions from special processing
    .ForEachCase(
        compliance => await complianceSystem.ReviewAsync(compliance),
        fraud => await fraudDetection.InvestigateAsync(fraud),
        currency => await currencyService.ConvertAsync(currency),
        vip => await vipProcessor.FastTrackAsync(vip)
    )
    .AllCases()
    .WriteCsv("special_transactions.csv");
```

### 5.3 DataFlow.Framework Layer

#### AsyncEnumerable<T> - Stream Union/Merger (IAsyncEnumerable)

Implements `IAsyncEnumerable<T>` and merges multiple child `IAsyncEnumerable<T>` sources into a single stream. It manages concurrent MoveNextAsync calls, synchronization, and source lifecycle during enumeration. It does not include built-in buffering; use opt-in buffering extensions when needed.

Key types:

```csharp
public enum UnifyErrorMode { FailFast, ContinueOnError }
public enum UnifyFairness { FirstAvailable, RoundRobin }

public sealed class UnifyOptions
{
    public UnifyErrorMode ErrorMode { get; init; } = UnifyErrorMode.FailFast;
    public UnifyFairness Fairness { get; init; } = UnifyFairness.FirstAvailable;
}
```

Construction and source registration:

```csharp
var unified = new AsyncEnumerable<MyEvent>(new UnifyOptions
{
    ErrorMode = UnifyErrorMode.ContinueOnError,
    Fairness  = UnifyFairness.RoundRobin
});

// Register sources before enumeration
unified.Unify(sourceA, "A")
       .Unify(sourceB, "B", evt => evt.IsImportant)
       .Unify(sourceC, "C");
 

// Optional: remove a source before starting
unified.Unlisten("B");
```

Enumeration semantics:

- The merger starts enumeration by creating async enumerators for each source and scheduling their MoveNextAsync calls.
- FirstAvailable: yields whichever source completes next.
- RoundRobin: cycles sources, awaiting or consuming whichever is ready to reduce starvation (still zero buffering).
- Error handling:
  - FailFast: any source MoveNextAsync exception fails the whole unified stream with a wrapped InvalidOperationException("DataFlow source 'name' failed.", ex).
  - ContinueOnError: drop the failing source and continue others.
- Per-source predicates are applied before yielding items.
- Once enumeration starts, the set of sources is frozen; Unify/Unlisten after start throws InvalidOperationException.
- For high-throughput or bursty producers, compose buffering explicitly via WithBoundedBuffer on each source or on the unified stream.

Example:

```csharp
await foreach (var item in unified)
{
    // process merged items
}
```

#### Regex and RegexTokenizer Classes - Stream-Aware Regex

Simplified regular expression utilities with **streaming support** and fluent syntax.

**Regex Constants:**

```csharp
public static readonly Regex NUMS = new(@"\d+");
public static readonly Regex ALPHAS = new(@"[a-zA-Z]+");
public static readonly Regex WORDS = new(@"\w+");
public static readonly Regex SPACES = new(@"\s+");
public static readonly Regex MAYBE_SPACES = new(@"\s*");
```

**Streaming Regex Example:**

```csharp
// Extract HTTP status codes from streaming log data
await liveAccessLogStream
    .Map($"HTTP/1.1\" {NUMS.As("StatusCode")} {NUMS.As("ResponseSize")}")
    .Cases("StatusCode")
    .SelectCase(code => 
        code.StartsWith("4") || code.StartsWith("5") ? 
        new ErrorResponse { Code = code, Timestamp = DateTime.Now } :
        new SuccessResponse { Code = code, Timestamp = DateTime.Now })
    .ForEachCase(error => await errorTracker.RecordAsync(error))
    .AllCases()
    .WriteJson("http_responses.json");
```

## 6. Stream Processing Deep Dive

### 6.1 Multi-Source Stream Architecture

DataFlow.NET excels at **multi-source stream processing**, where data arrives from various sources simultaneously and needs unified processing.

#### Heterogeneous Stream Processing

```csharp
public class MultiSourceProcessor
{
    private readonly Channel<OrderEvent> _orderChannel = Channel.CreateUnbounded<OrderEvent>();
    private readonly Channel<InventoryEvent> _inventoryChannel = Channel.CreateUnbounded<InventoryEvent>();
    private readonly Channel<CustomerEvent> _customerChannel = Channel.CreateUnbounded<CustomerEvent>();
  
    public async Task ProcessBusinessEvents()
    {
        // Create separate mergers for different event types
        var orderStream = new AsyncEnumerable<OrderEvent>()
            .Unify(_orderChannel.Reader.ReadAllAsync(), "orders");
        var inventoryStream = new AsyncEnumerable<InventoryEvent>()
            .Unify(_inventoryChannel.Reader.ReadAllAsync(), "inventory");
        var customerStream = new AsyncEnumerable<CustomerEvent>()
            .Unify(_customerChannel.Reader.ReadAllAsync(), "customers");
  
        // Process each stream type with specialized logic
        var orderTask = ProcessOrderEvents(orderStream);
        var inventoryTask = ProcessInventoryEvents(inventoryStream);
        var customerTask = ProcessCustomerEvents(customerStream);

        // Run all processors concurrently
        await Task.WhenAll(orderTask, inventoryTask, customerTask);
    }
  
    private async Task ProcessOrderEvents(IAsyncEnumerable<OrderEvent> stream)
    {
        await stream
            .Cases(
                order => order.Type == OrderType.HighValue,
                order => order.Type == OrderType.International,
                order => order.Type == OrderType.Subscription,
                order => order.Customer.IsVIP
            )
            .SelectCase(
                highValue => new HighValueOrderAlert 
                { 
                    OrderId = highValue.OrderId, 
                    Amount = highValue.Amount,
                    RequiresApproval = true
                },
                international => new InternationalOrderProcess 
                { 
                    OrderId = international.OrderId,
                    RequiresCurrencyConversion = true,
                    TaxCalculationNeeded = true
                },
                subscription => new SubscriptionOrderProcess 
                { 
                    OrderId = subscription.OrderId,
                    RecurringSetupNeeded = true
                },
                vip => new VIPOrderProcess 
                { 
                    OrderId = vip.OrderId,
                    PriorityProcessing = true,
                    PersonalizedService = true
                },
                standard => new StandardOrderProcess { OrderId = standard.OrderId }
            )
            .ForEachCase(
                highValue => await approvalSystem.RequestApprovalAsync(highValue),
                international => await internationalProcessor.ProcessAsync(international),
                subscription => await subscriptionManager.SetupRecurringAsync(subscription),
                vip => await vipProcessor.PrioritizeAsync(vip),
                standard => await standardQueue.EnqueueAsync(standard)
            )
            .AllCases()
            .WriteJson($"order_processing_{DateTime.Now:yyyyMMdd}.json");
    }
  
    private async Task ProcessInventoryEvents(IAsyncEnumerable<InventoryEvent> stream)
    {
        await stream
            .Cases(
                inv => inv.NewQuantity == 0,           // Out of stock
                inv => inv.NewQuantity <= inv.ReorderLevel,  // Low stock
                inv => inv.ChangeType == InventoryChangeType.Damaged  // Damaged goods
            )
            .SelectCase(
                outOfStock => new OutOfStockAlert 
                { 
                    ProductId = outOfStock.ProductId,
                    LastQuantity = outOfStock.PreviousQuantity,
                    UrgentReorderNeeded = true
                },
                lowStock => new LowStockWarning 
                { 
                    ProductId = lowStock.ProductId,
                    CurrentQuantity = lowStock.NewQuantity,
                    ReorderLevel = lowStock.ReorderLevel,
                    SuggestedReorderQuantity = lowStock.OptimalQuantity
                },
                damaged => new DamagedGoodsReport 
                { 
                    ProductId = damaged.ProductId,
                    DamagedQuantity = damaged.PreviousQuantity - damaged.NewQuantity,
                    RequiresInvestigation = true
                },
                normal => new InventoryUpdate 
                { 
                    ProductId = normal.ProductId,
                    NewQuantity = normal.NewQuantity
                }
            )
            .ForEachCase(
                outOfStock => await purchasingSystem.CreateUrgentOrderAsync(outOfStock),
                lowStock => await purchasingSystem.CreateReorderSuggestionAsync(lowStock),
                damaged => await qualityControl.InvestigateAsync(damaged),
                normal => await inventoryDB.UpdateAsync(normal)
            )
            .AllCases()
            .WriteCsv($"inventory_changes_{DateTime.Now:yyyyMMdd}.csv");
    }
}
```

#### Advanced Stream Merging with Conditional Processing

```csharp

    // Create conditional mergers for different priority levels
    var criticalEventsMerger = new AsyncEnumerable<SystemEvent>()
        .Unify(webServerEvents.Where(evt => evt.Severity == EventSeverity.Critical), "web")
        .Unify(databaseEvents.Where(evt => evt.Severity == EventSeverity.Critical), "db")
        .Unify(authEvents.Where(evt => evt.Severity == EventSeverity.Critical), "auth")
        .Unify(paymentEvents.Where(evt => evt.Severity == EventSeverity.Critical), "pay");
  
    var warningEventsMerger = new AsyncEnumerable<SystemEvent>()
        .Unify(webServerEvents.Where(evt => evt.Severity == EventSeverity.Warning), "web")
        .Unify(databaseEvents.Where(evt => evt.Severity == EventSeverity.Warning), "db")
        .Unify(authEvents.Where(evt => evt.Severity == EventSeverity.Warning), "auth")
        .Unify(paymentEvents.Where(evt => evt.Severity == EventSeverity.Warning), "pay");
  
    var infoEventsMerger = new AsyncEnumerable<SystemEvent>()
        .Unify(webServerEvents.Where(evt => evt.Severity == EventSeverity.Info), "web")
        .Unify(databaseEvents.Where(evt => evt.Severity == EventSeverity.Info), "db")
        .Unify(authEvents.Where(evt => evt.Severity == EventSeverity.Info), "auth")
        .Unify(paymentEvents.Where(evt => evt.Severity == EventSeverity.Info), "pay");

    criticalEventsMerger.ForEach(/* evt=> critical events treatment */);
    warningEventsMerger.ForEach(/* evt=> warnng events treatment */);
    infoEventsMerger.ForEach(/* evt=> info events treatment */);

    ...

    private async Task ProcessCriticalEvents(IAsyncEnumerable<SystemEvent> stream)
    {
        await stream
            .Cases(
                evt => evt.Source == "Database",
                evt => evt.Source == "Payment",
                evt => evt.Source == "Authentication"
            )
            .SelectCase(
                dbEvent => new DatabaseCriticalAlert 
                { 
                    Message = dbEvent.Message,
                    RequiresImmediateAction = true,
                    EscalateToDBAdmin = true
                },
                paymentEvent => new PaymentCriticalAlert 
                { 
                    Message = paymentEvent.Message,
                    RequiresImmediateAction = true,
                    EscalateToFinanceTeam = true
                },
                authEvent => new SecurityCriticalAlert 
                { 
                    Message = authEvent.Message,
                    RequiresImmediateAction = true,
                    EscalateToSecurityTeam = true
                },
                otherEvent => new GeneralCriticalAlert 
                { 
                    Message = otherEvent.Message,
                    Source = otherEvent.Source
                }
            )
            .ForEachCase(
                dbAlert => await emergencyResponse.EscalateToDBTeamAsync(dbAlert),
                paymentAlert => await emergencyResponse.EscalateToFinanceAsync(paymentAlert),
                authAlert => await emergencyResponse.EscalateToSecurityAsync(authAlert),
                generalAlert => await emergencyResponse.EscalateToOpsAsync(generalAlert)
            )
            .AllCases()
            .WriteText($"critical_events_{DateTime.Now:yyyyMMdd_HHmmss}.log");
    }
}
```

### 6.2 Performance Optimization for Streaming

#### Channel Configuration and Backpressure Management

```csharp
public class OptimizedStreamProcessor
{
    private readonly BoundedChannelOptions _channelOptions = new(1000)
    {
        FullMode = BoundedChannelFullMode.Wait,
        SingleReader = false,
        SingleWriter = false,
        AllowSynchronousContinuations = false
    };
  
    public async Task ProcessHighThroughputStream()
    {
        // Configure high-performance channels
        var highThroughputChannel = Channel.CreateBounded<DataPoint>(_channelOptions);

        // Producer writes to channel.Writer ...
  
        // Process with optimized pipeline
        await highThroughputChannel.Reader.ReadAllAsync()
            .Where(data => data.IsValid && data.Timestamp > DateTime.Now.AddMinutes(-1))
            .Cases(
                data => data.Priority == Priority.High,
                data => data.Priority == Priority.Medium
            )
            .SelectCase(
                high => ProcessHighPriority(high),
                medium => ProcessMediumPriority(medium),
                low => ProcessLowPriority(low)
            )
            .ForEachCase(
                high => await highPriorityProcessor.ProcessAsync(high),
                medium => await mediumPriorityProcessor.ProcessAsync(medium),
                low => await lowPriorityProcessor.ProcessAsync(low)
            )
            .AllCases()
            .WriteCsv("processed_data.csv");
    }
}
```

## 7. API Reference

### 7.1 Core Types

#### AsyncEnumerable<T>

```csharp
public sealed class AsyncEnumerable<T> : IAsyncEnumerable<T>
{
    public AsyncEnumerable(UnifyOptions? options = null);
    public AsyncEnumerable<T> Unify(IAsyncEnumerable<T> source, string name, Func<T, bool>? predicate = null);
    public bool Unlisten(string name);
    public IAsyncEnumerator<T> GetAsyncEnumerator(CancellationToken cancellationToken = default);
}

public enum UnifyErrorMode { FailFast, ContinueOnError }
public enum UnifyFairness { FirstAvailable, RoundRobin }

public sealed class UnifyOptions
{
    public UnifyErrorMode ErrorMode { get; init; } = UnifyErrorMode.FailFast;
    public UnifyFairness Fairness { get; init; } = UnifyFairness.FirstAvailable;
}
```

### 7.2 Extension Method Categories

#### Unified Processing Extensions

```csharp
// Core categorization methods (both sync and async versions)
public static IEnumerable<(int category, T item)> Cases<T>(
    this IEnumerable<T> items, params Func<T, bool>[] filters);

public static IAsyncEnumerable<(int category, T item)> Cases<T>(
    this IAsyncEnumerable<T> items, params Func<T, bool>[] filters);

// Transformation methods
public static IEnumerable<(int category, T item, R newItem)> SelectCase<T, R>(
    this IEnumerable<(int category, T item)> items, params Func<T, R>[] selectors);

public static IAsyncEnumerable<(int category, T item, R newItem)> SelectCase<T, R>(
    this IAsyncEnumerable<(int category, T item)> items, params Func<T, R>[] selectors);

// Action methods
public static IEnumerable<(int category, T item, R newItem)> ForEachCase<T, R>(
    this IEnumerable<(int category, T item, R newItem)> items, params Action<R>[] actions);

public static IAsyncEnumerable<(int category, T item, R newItem)> ForEachCase<T, R>(
    this IAsyncEnumerable<(int category, T item, R newItem)> items, params Func<R, Task>[] actions);

// Result extraction
public static IEnumerable<R> AllCases<T, R>(
    this IEnumerable<(int category, T item, R newItem)> items);

public static IAsyncEnumerable<R> AllCases<T, R>(
    this IAsyncEnumerable<(int category, T item, R newItem)> items);
```

#### Utility Extensions

```csharp

// Producer/consumer and buffering helpers
public static async IAsyncEnumerable<T> Async<T>(this IEnumerable<T> items, long yieldThresholdMs = 15, [EnumeratorCancellation] CancellationToken ct = default);
public static async IAsyncEnumerable<T> BufferAsync<T>(this IEnumerable<T> source, long yieldThresholdMs = 15, bool runOnBackgroundThread = false, BoundedChannelOptions? options = null, [EnumeratorCancellation] CancellationToken ct = default);
public static IAsyncEnumerable<T> WithBoundedBuffer<T>(this IAsyncEnumerable<T> source, BoundedChannelOptions? options = null, CancellationToken ct = default);
public static IAsyncEnumerable<T> WithBoundedBuffer<T>(this IAsyncEnumerable<T> source, int capacity, BoundedChannelFullMode fullMode = BoundedChannelFullMode.Wait, CancellationToken ct = default);

// Conditional processing
public static IEnumerable<T> Until<T>(this IEnumerable<T> items, Func<T, bool> condition);
public static IAsyncEnumerable<T> Until<T>(this IAsyncEnumerable<T> items, Func<T, bool> condition);

// Debugging and monitoring
public static IEnumerable<T> Spy<T>(this IEnumerable<T> items, Action<T> action);
public static IAsyncEnumerable<T> Spy<T>(this IAsyncEnumerable<T> items, Func<T, Task> action);

// String building
public static string BuildString<T>(this IEnumerable<T> items, Func<T, string> selector, string separator = "");
public static Task<string> BuildStringAsync<T>(this IAsyncEnumerable<T> items, Func<T, string> selector, string separator = "");

// File operations
public static void WriteTextSync(this IEnumerable<string> lines, string path);
public static Task WriteText(this IEnumerable<string> lines, string path);
public static Task WriteText(this IAsyncEnumerable<string> lines, string path);

public static void WriteCsvSync<T>(this IEnumerable<T> records, string path, bool withTitle = true, string separator = ",");
public static ValueTask WriteCsv<T>(this IEnumerable<T> records, string path, bool withTitle = true, string separator = ",");
public static ValueTask WriteCsv<T>(this IAsyncEnumerable<T> records, string path, bool withTitle = true, string separator = ",");

public static void WriteJsonSync<T>(this IEnumerable<T> items, string path);
public static ValueTask WriteJson<T>(this IEnumerable<T> items, string path);
public static Task WriteJson<T>(this IAsyncEnumerable<T> items, string path);

public static void WriteYamlSync<T>(this IEnumerable<T> items, string path);
public static Task WriteYaml<T>(this IAsyncEnumerable<T> items, string path);

```

## 8. Advanced Topics

### 8.1 Lazy Evaluation Strategy

DataFlow.NET uses **lazy evaluation** throughout the pipeline for both batch and streaming scenarios:

```csharp
// This pipeline doesn't execute until enumerated
var pipeline = Read.Csv<Order>("orders.csv")
    .Cases(IsUrgent, IsStandard)
    .SelectCase(ProcessUrgent, ProcessStandard)
    .ForEachCase(LogUrgent, LogStandard);

// Execution happens here (batch)
var results = pipeline.AllCases().ToList();

// Or here (streaming)
await foreach (var result in pipeline.AllCases())
{
    // Process each result as it becomes available
}
```

### 8.2 Async Processing Patterns

The framework supports multiple async processing patterns:

#### Fire-and-Forget Processing

```csharp
public async Task ProcessFireAndForget()
{
    // Set up background processing
    _ = Task.Run(async () =>
    {
       // Producer writes into a channel from anywhere
        var channel = Channel.CreateUnbounded<LogEntry>();
        var merger = new AsyncEnumerable<LogEntry>().Unify(channel.Reader.ReadAllAsync(), "logs");
        await merger
            .Cases(IsError, IsWarning)
            .SelectCase(ProcessError, ProcessWarning)
            .ForEachCase(
                error => _ = Task.Run(() => alertSystem.SendAsync(error)),
                warning => _ = Task.Run(() => logger.LogAsync(warning))
            )
            .AllCases()
            .WriteText("background_processing.log");
    });
  
    // Continue with main processing while background task runs
    // Example producer path (not shown): channel.Writer.TryWrite(...)
}
```

#### Backpressure-Aware Processing

```csharp
public async Task ProcessWithBackpressure()
{
    var options = new BoundedChannelOptions(100)
    {
        FullMode = BoundedChannelFullMode.Wait,  // Apply backpressure
        SingleReader = true,
        SingleWriter = false
    };
  
    var channel = Channel.CreateBounded<DataPoint>(options);

  
    // Consumer respects backpressure
    await channel.Reader.ReadAllAsync()
        .Cases(IsHighPriority, IsMediumPriority)
        .SelectCase(
            high => ProcessSlowly(high),      // Intentionally slow processing
            medium => ProcessNormally(medium),
            low => ProcessQuickly(low)
        )
        .AllCases()
        .WriteCsv("backpressure_processed.csv");
}
```

### 8.3 Memory Management and Resource Cleanup

#### Proper Disposal Patterns

```csharp
public class ResourceAwareProcessor : IAsyncDisposable
{
    private readonly List<StreamWriter> _writers;
  
    public ResourceAwareProcessor()
    {
        _writers = new List<StreamWriter>();
    }
  
    public async Task ProcessWithProperCleanup()
    {
        var errorWriter = new StreamWriter("errors.txt");
        var warningWriter = new StreamWriter("warnings.txt");
        _writers.AddRange(new[] { errorWriter, warningWriter });
  
        try
        {
            // Example: merge two channels into one stream for processing
            var ch1 = Channel.CreateUnbounded<SensorData>();
            var ch2 = Channel.CreateUnbounded<SensorData>();
            var merger = new AsyncEnumerable<SensorData>()
                .Unify(ch1.Reader.ReadAllAsync(), "ch1")
                .Unify(ch2.Reader.ReadAllAsync(), "ch2");

            await _merger
                .Cases(
                    data => data.IsError,
                    data => data.IsWarning
                )
                .SelectCase(
                    error => $"ERROR: {error.Message}",
                    warning => $"WARNING: {warning.Message}",
                    info => $"INFO: {info.Message}"
                )
                .ForEachCase(
                    error => errorWriter.WriteLine(error),
                    warning => warningWriter.WriteLine(warning),
                    info => { /* Ignored - supra category */ }
                )
                .AllCases()
                .WriteText("all_processed.txt");
        }
        finally
        {
            // Ensure proper cleanup
            foreach (var writer in _writers)
            {
                await writer.FlushAsync();
                writer.Dispose();
            }
        }
    }
  
    public async ValueTask DisposeAsync()
    {
  
        foreach (var writer in _writers)
        {
            if (writer != null)
            {
                await writer.FlushAsync();
                writer.Dispose();
            }
        }
  
        _writers.Clear();
    }
}
```

### 8.4 Regular Expression Integration with Streaming

#### Stream-Aware Pattern Matching

```csharp
public async Task ProcessLogStreamWithRegex()
{
    // Define patterns for different log formats
    var webLogPattern = $"{WORDS.As("Method")} {WORDS.As("Path")} HTTP/1.1\" {NUMS.As("Status")} {NUMS.As("Size")}";
    var dbLogPattern = $"DB Query: {WORDS.As("Operation")} on {WORDS.As("Table")} took {NUMS.As("Duration")}ms";
    var authLogPattern = $"Auth: User {WORDS.As("Username")} {WORDS.As("Action")} from {WORDS.As("IP")}";
  
    await liveLogStream
        .Map(webLogPattern, dbLogPattern, authLogPattern)
        .Cases(
            "Status",     // Web logs
            "Duration",   // DB logs  
            "Username"    // Auth logs
        )
        .SelectCase(
            status => new WebLogEntry 
            { 
                StatusCode = int.Parse(status),
                IsError = status.StartsWith("4") || status.StartsWith("5")
            },
            duration => new DatabaseLogEntry 
            { 
                QueryDuration = int.Parse(duration),
                IsSlow = int.Parse(duration) > 1000
            },
            username => new AuthLogEntry 
            { 
                Username = username,
                Timestamp = DateTime.Now
            },
            unmatched => new UnknownLogEntry 
            { 
                RawMessage = unmatched,
                RequiresReview = true
            }
        )
        .ForEachCase(
            webLog => await webAnalytics.RecordAsync(webLog),
            dbLog => await dbMonitor.RecordAsync(dbLog),
            authLog => await securityMonitor.RecordAsync(authLog),
            unknown => await reviewQueue.EnqueueAsync(unknown)
        )
        .AllCases()
        .WriteJson("categorized_logs.json");
}
```

## 9. Best Practices

### 9.1 Unified Processing Design Patterns

#### 1. **Write Processing Logic Once, Deploy Everywhere**

```csharp
// Define reusable processing logic
public static class OrderProcessingLogic
{
    public static async Task<IEnumerable<ProcessedOrder>> ProcessOrders<T>(T orderSource)
        where T : IAsyncEnumerable<Order>
    {
        return await orderSource
            .Cases(
                order => order.Amount > 1000,
                order => order.IsInternational,
                order => order.Customer.IsVIP
            )
            .SelectCase(
                highValue => ProcessHighValueOrder(highValue),
                international => ProcessInternationalOrder(international),
                vip => ProcessVIPOrder(vip),
                standard => ProcessStandardOrder(standard)
            )
            .ForEachCase(
                highValue => await complianceSystem.ReviewAsync(highValue),
                international => await currencyService.ProcessAsync(international),
                vip => await vipService.PrioritizeAsync(vip),
                standard => await standardQueue.EnqueueAsync(standard)
            )
            .AllCases()
            .ToListAsync();
    }
}

// Use with batch data
var batchResults = await OrderProcessingLogic.ProcessOrders(
    Read.Csv<Order>("historical_orders.csv")
);

// Use with streaming data (IDENTICAL LOGIC!)
var streamResults = await OrderProcessingLogic.ProcessOrders(liveOrderStream);
```

#### 2. **Leverage the Supra Category Pattern for Robust Processing**

```csharp
// ‚úÖ Good: Express intent clearly, ignore unrecognized data gracefully
public async Task ProcessBusinessEvents(IAsyncEnumerable<BusinessEvent> events)
{
    await events
        .Cases(
            evt => evt.Type == "CustomerRegistration",
            evt => evt.Type == "OrderPlaced",
            evt => evt.Type == "PaymentReceived"
            // New event types (ProductViewed, CartAbandoned, etc.) 
            // automatically become supra category - won't break processing
        )
        .SelectCase(
            registration => ProcessCustomerRegistration(registration),
            order => ProcessOrderPlacement(order),
            payment => ProcessPaymentReceived(payment)
            // No selector for supra category = graceful ignoring
        )
        .Where(x => x.newItem != null)  // Filter out ignored events
        .ForEachCase(
            registration => await crmSystem.AddCustomerAsync(registration),
            order => await fulfillmentSystem.ProcessAsync(order),
            payment => await accountingSystem.RecordAsync(payment)
        )
        .AllCases()
        .WriteJson("processed_business_events.json");
}
```

#### 3. **Use Conditional Mergers for Performance**

```csharp
// ‚úÖ Good: Filter at merger level for better performance
var criticalEventsMerger = new AsyncEnumerable<SystemEvent>(
    condition: evt => evt.Severity >= EventSeverity.Warning,  // Pre-filter
    webServerEvents, databaseEvents, authEvents
);

// Only critical events flow through the processing pipeline
await criticalEventsMerger
    .Cases(IsError, IsWarning)
    .SelectCase(ProcessError, ProcessWarning)
    .AllCases()
    .WriteText("critical_events.log");
```

### 9.2 Performance Optimization

#### 1. **Stream Processing Best Practices**

```csharp
// ‚úÖ Good: Process data as it arrives, don't buffer unnecessarily
await dataStream
    .Cases(IsHighPriority, IsMediumPriority)
    .SelectCase(ProcessHigh, ProcessMedium, ProcessLow)
    .ForEachCase(
        high => await highPriorityHandler.ProcessAsync(high),
        medium => await mediumPriorityHandler.ProcessAsync(medium),
        low => await lowPriorityHandler.ProcessAsync(low)
    )
    .AllCases()
    .WriteCsv("results.csv");  // Streaming write - no buffering

// ‚ùå Bad: Don't buffer entire stream in memory
var allResults = await dataStream.ToListAsync();  // Loads everything into memory
foreach (var result in allResults) { /* process */ }
```

#### 2. **Channel Configuration for High-Throughput Scenarios**

```csharp
// ‚úÖ Good: Configure channels appropriately for your scenario
public class HighThroughputProcessor
{
    private readonly BoundedChannelOptions _options = new(10000)
    {
        FullMode = BoundedChannelFullMode.Wait,        // Backpressure
        SingleReader = true,                           // Performance optimization
        SingleWriter = false,                          // Multiple producers
        AllowSynchronousContinuations = false         // Prevent blocking
    };
  
    public async Task ProcessHighThroughput()
    {
        var channel = Channel.CreateBounded<DataPoint>(_options);
        var publisher = new DataPublisher<DataPoint>();
  
        publisher.AddWriter(channel.Writer);
  
        await channel.Reader.ReadAllAsync()
            .Cases(IsUrgent, IsStandard)
            .SelectCase(ProcessUrgent, ProcessStandard)
            .AllCases()
            .WriteCsv("high_throughput_results.csv");
    }
}
```

### 9.3 Code Organization

#### 1. **Separate Concerns with Layer Architecture**

```csharp
// Data Layer - Handle data access
public static class DataSources
{
    public static IAsyncEnumerable<Order> GetLiveOrders() => 
        new AsyncEnumerable<Order>()
            .Unify(GetLiveOrdersAsync(), "live-orders");
   
  
    public static IAsyncEnumerable<Order> GetHistoricalOrders() => 
        Read.Csv<Order>("orders.csv");
}

// Extensions Layer - Define processing logic
public static class OrderProcessingExtensions
{
    public static IAsyncEnumerable<(int category, Order order, ProcessedOrder result)> ProcessOrderCategories(
        this IAsyncEnumerable<Order> orders)
    {
        return orders
            .Cases(IsHighValue, IsInternational, IsVIP)
            .SelectCase(ProcessHighValue, ProcessInternational, ProcessVIP, ProcessStandard);
    }
}

// Framework Layer - Orchestrate processing
public class OrderProcessingService
{
    public async Task ProcessLiveOrders()
    {
        await DataSources.GetLiveOrders()
            .ProcessOrderCategories()
            .ForEachCase(
                highValue => await complianceService.ReviewAsync(highValue),
                international => await currencyService.ProcessAsync(international),
                vip => await vipService.HandleAsync(vip),
                standard => await standardService.ProcessAsync(standard)
            )
            .AllCases()
            .WriteJson("processed_orders.json");
    }
}
```

#### 2. **Create Reusable Processing Components**

```csharp
// Reusable processing patterns
public static class CommonProcessingPatterns
{
    public static IAsyncEnumerable<T> ProcessWithRetry<T>(
        this IAsyncEnumerable<T> items,
        Func<T, Task<T>> processor,
        int maxRetries = 3)
    {
        return items.SelectAwait(async item =>
        {
            for (int retry = 0; retry <= maxRetries; retry++)
            {
                try
                {
                    return await processor(item);
                }
                catch when (retry < maxRetries)
                {
                    await Task.Delay(TimeSpan.FromSeconds(Math.Pow(2, retry)));
                }
            }
            throw new InvalidOperationException($"Failed after {maxRetries} retries");
        });
    }
  
    public static IAsyncEnumerable<T> ProcessWithCircuitBreaker<T>(
        this IAsyncEnumerable<T> items,
        Func<T, Task<T>> processor,
        int failureThreshold = 5)
    {
        var failureCount = 0;
        var isOpen = false;
  
        return items.SelectAwait(async item =>
        {
            if (isOpen)
                throw new InvalidOperationException("Circuit breaker is open");
          
            try
            {
                var result = await processor(item);
                failureCount = 0;  // Reset on success
                return result;
            }
            catch
            {
                failureCount++;
                if (failureCount >= failureThreshold)
                    isOpen = true;
                throw;
            }
        });
    }
}
```

## 10. Performance Guide

### 10.1 Benchmarking Results

#### Memory Usage Comparison

```
Scenario: Processing 1M records

Traditional Approach:
- Memory Usage: ~800MB (loads entire dataset)
- Processing Time: 2.3 seconds
- Peak Memory: 1.2GB

DataFlow.NET Batch:
- Memory Usage: ~15MB (constant)
- Processing Time: 1.8 seconds  
- Peak Memory: 25MB

DataFlow.NET Streaming:
- Memory Usage: ~12MB (constant)
- Processing Time: N/A (real-time)
- Latency: <50ms per item
```

#### Throughput Benchmarks

```
Stream Processing Throughput:
- Single source: ~50,000 items/second
- 4 merged sources: ~180,000 items/second
- 10 merged sources: ~400,000 items/second

Cases Pattern Performance:
- 2 categories: 98% of baseline performance
- 5 categories: 95% of baseline performance
- 10 categories: 92% of baseline performance

Supra Category Overhead:
- Negligible (<1% performance impact)
- Memory overhead: ~8 bytes per item
```

### 10.2 Optimization Techniques

#### 1. **Pipeline Optimization**

```csharp
// ‚úÖ Good: Combine operations to reduce iterations
await dataStream
    .Cases(IsType1, IsType2)
    .SelectCase(Transform1, Transform2, Transform3)
    .ForEachCase(Action1, Action2, Action3)
    .AllCases()
    .WriteCsv("results.csv");

// ‚ùå Bad: Multiple separate iterations
var categorized = await dataStream.Cases(IsType1, IsType2).ToListAsync();
var transformed = categorized.SelectCase(Transform1, Transform2, Transform3);
var processed = transformed.ForEachCase(Action1, Action2, Action3);
await processed.AllCases().WriteCsv("results.csv");
```

#### 2. **Memory Management**

```csharp
// ‚úÖ Good: Use streaming for large datasets
await Read.Csv<LargeRecord>("huge_file.csv")
    .Cases(IsImportant, IsUrgent)
    .SelectCase(ProcessImportant, ProcessUrgent, ProcessNormal)
    .AllCases()
    .WriteCsv("processed.csv");

// ‚ùå Bad: Loading everything into memory
var allRecords = Read.Csv<LargeRecord>("huge_file.csv").ToList();
var processed = allRecords.Cases(IsImportant, IsUrgent)
    .SelectCase(ProcessImportant, ProcessUrgent, ProcessNormal)
    .AllCases()
    .ToList();
```

#### 3. **Async Optimization**

```csharp
// ‚úÖ Good: Proper async/await usage
public async Task ProcessStreamsOptimally()
{
    var stream1Task = ProcessStream1();
    var stream2Task = ProcessStream2();
    var stream3Task = ProcessStream3();
  
    await Task.WhenAll(stream1Task, stream2Task, stream3Task);
}

private async Task ProcessStream1()
{
    await dataStream1
        .Cases(predicate1, predicate2)
        .SelectCase(transform1, transform2)
        .ForEachCase(
            async item => await processor1.ProcessAsync(item),
            async item => await processor2.ProcessAsync(item)
        )
        .AllCases()
        .WriteText("stream1_results.txt");
}
```

### 10.3 Common Performance Pitfalls

#### 1. **Avoid Premature Materialization**

```csharp
// ‚ùå Bad: Breaks lazy evaluation
var results = await dataStream.Cases(pred1, pred2).ToListAsync();
var processed = results.SelectCase(transform1, transform2).ToList();

// ‚úÖ Good: Maintain lazy evaluation throughout
await dataStream
    .Cases(pred1, pred2)
    .SelectCase(transform1, transform2)
    .AllCases()
    .WriteCsv("results.csv");
```

#### 2. **Avoid Synchronous Operations in Async Streams**

```csharp
// ‚ùå Bad: Blocking async stream processing
await asyncStream
    .ForEachCase(
        item => synchronousProcessor.Process(item),  // Blocks async flow
        item => anotherSyncProcessor.Process(item)
    )
    .AllCases()
    .WriteText("results.txt");

// ‚úÖ Good: Use async operations throughout
await asyncStream
    .ForEachCase(
        async item => await asyncProcessor.ProcessAsync(item),
        async item => await anotherAsyncProcessor.ProcessAsync(item)
    )
    .AllCases()
    .WriteText("results.txt");
```

#### 3. **Avoid Creating Excessive Intermediate Collections**

```csharp
// ‚ùå Bad: Creates multiple intermediate collections
var step1 = dataStream.Cases(pred1, pred2).ToListAsync();
var step2 = step1.SelectCase(transform1, transform2).ToListAsync();
var step3 = step2.ForEachCase(action1, action2).ToListAsync();

// ‚úÖ Good: Single pipeline with no intermediate collections
await dataStream
    .Cases(pred1, pred2)
    .SelectCase(transform1, transform2)
    .ForEachCase(action1, action2)
    .AllCases()
    .WriteCsv("results.csv");
```

#### 4. **Proper Resource Management**

```csharp
// ‚ùå Bad: Not disposing resources properly
public async Task ProcessWithoutProperCleanup()
{
    var channel = Channel.CreateUnbounded<Data>();
    var merger = new AsyncEnumerable<Data>().Unify(channel.Reader.ReadAllAsync(), "data");

    await merger.Cases(pred1, pred2).AllCases().WriteText("output.txt");
    // Resources not disposed - potential memory leaks
}

// ‚úÖ Good: Proper resource disposal
public async Task ProcessWithProperCleanup()
{
    var channel = Channel.CreateUnbounded<Data>();
    var reader = channel.Reader;
    var merger = new AsyncEnumerable<Data>().Unify(reader.ReadAllAsync(), "data");
  
    await merger
        .Cases(pred1, pred2)
        .SelectCase(transform1, transform2)
        .AllCases()
        .WriteText("output.txt");
    // Resources automatically disposed
}
```

### 10.4 Advanced Performance Patterns

#### 1. **Parallel Processing with Partitioning**

```csharp
public async Task ProcessInParallel()
{
    const int partitionCount = Environment.ProcessorCount;
  
    // Partition stream for parallel processing
    var partitions = Partitioner.Create(dataSource, true)
        .GetPartitions(partitionCount)
        .Select(partition => ProcessPartition(partition));
  
    await Task.WhenAll(partitions);
}

private async Task ProcessPartition(IEnumerator<DataItem> partition)
{
    var items = EnumeratePartition(partition);
  
    await items
        .Cases(IsHighPriority, IsMediumPriority)
        .SelectCase(ProcessHigh, ProcessMedium, ProcessLow)
        .ForEachCase(
            async high => await highPriorityProcessor.ProcessAsync(high),
            async medium => await mediumPriorityProcessor.ProcessAsync(medium),
            async low => await lowPriorityProcessor.ProcessAsync(low)
        )
        .AllCases()
        .WriteCsv($"partition_results_{Thread.CurrentThread.ManagedThreadId}.csv");
}

private async IAsyncEnumerable<DataItem> EnumeratePartition(IEnumerator<DataItem> partition)
{
    while (partition.MoveNext())
    {
        yield return partition.Current;
    }
}
```

#### 2. **Batched Processing for High-Throughput Scenarios**

```csharp
public async Task ProcessInBatches()
{
    const int batchSize = 1000;
  
    await dataStream
        .Buffer(batchSize)  // Process in batches of 1000
        .SelectAwait(async batch => 
        {
            // Process entire batch at once for efficiency
            return await batch.ToAsyncEnumerable()
                .Cases(IsType1, IsType2, IsType3)
                .SelectCase(
                    ProcessType1Batch,
                    ProcessType2Batch, 
                    ProcessType3Batch,
                    ProcessDefaultBatch
                )
                .ForEachCase(
                    async type1Batch => await type1Processor.ProcessBatchAsync(type1Batch),
                    async type2Batch => await type2Processor.ProcessBatchAsync(type2Batch),
                    async type3Batch => await type3Processor.ProcessBatchAsync(type3Batch),
                    async defaultBatch => await defaultProcessor.ProcessBatchAsync(defaultBatch)
                )
                .AllCases()
                .ToListAsync();
        })
        .SelectMany(batch => batch.ToAsyncEnumerable())
        .WriteCsv("batched_results.csv");
}
```

#### 3. **Circuit Breaker Pattern for Resilient Processing**

```csharp
public class ResilientStreamProcessor
{
    private readonly CircuitBreakerOptions _options = new()
    {
        FailureThreshold = 5,
        RecoveryTimeout = TimeSpan.FromMinutes(1),
        SamplingDuration = TimeSpan.FromSeconds(30)
    };
  
    public async Task ProcessWithCircuitBreaker()
    {
        var circuitBreaker = new CircuitBreaker(_options);
  
        await dataStream
            .Cases(IsHighRisk, IsMediumRisk)
            .SelectCase(
                async highRisk => await circuitBreaker.ExecuteAsync(() => ProcessHighRisk(highRisk)),
                async mediumRisk => await circuitBreaker.ExecuteAsync(() => ProcessMediumRisk(mediumRisk)),
                async lowRisk => await ProcessLowRisk(lowRisk)  // No circuit breaker for low risk
            )
            .ForEachCase(
                async result => await resultProcessor.ProcessAsync(result),
                async result => await resultProcessor.ProcessAsync(result),
                async result => await resultProcessor.ProcessAsync(result)
            )
            .AllCases()
            .WriteJson("resilient_results.json");
    }
}
```

### 10.5 Monitoring and Observability

#### 1. **Performance Monitoring**

```csharp
public async Task ProcessWithMonitoring()
{
    var stopwatch = Stopwatch.StartNew();
    var processedCount = 0;
    var errorCount = 0;
  
    await dataStream
        .Spy(async item => 
        {
            processedCount++;
            if (processedCount % 1000 == 0)
            {
                Console.WriteLine($"Processed {processedCount} items in {stopwatch.Elapsed}");
                Console.WriteLine($"Rate: {processedCount / stopwatch.Elapsed.TotalSeconds:F2} items/second");
            }
        })
        .Cases(IsValid, IsWarning, IsError)
        .SelectCase(
            valid => ProcessValid(valid),
            warning => ProcessWarning(warning),
            error => { errorCount++; return ProcessError(error); },
            unknown => ProcessUnknown(unknown)
        )
        .ForEachCase(
            async valid => await validProcessor.ProcessAsync(valid),
            async warning => await warningProcessor.ProcessAsync(warning),
            async error => await errorProcessor.ProcessAsync(error),
            async unknown => await unknownProcessor.ProcessAsync(unknown)
        )
        .AllCases()
        .Spy(async result => 
        {
            // Log final statistics
            if (processedCount % 10000 == 0)
            {
                await metricsLogger.LogAsync(new ProcessingMetrics
                {
                    ProcessedCount = processedCount,
                    ErrorCount = errorCount,
                    ProcessingRate = processedCount / stopwatch.Elapsed.TotalSeconds,
                    ElapsedTime = stopwatch.Elapsed
                });
            }
        })
        .WriteCsv("monitored_results.csv");
  
    Console.WriteLine($"Final Statistics:");
    Console.WriteLine($"Total Processed: {processedCount}");
    Console.WriteLine($"Total Errors: {errorCount}");
    Console.WriteLine($"Total Time: {stopwatch.Elapsed}");
    Console.WriteLine($"Average Rate: {processedCount / stopwatch.Elapsed.TotalSeconds:F2} items/second");
}
```

#### 2. **Health Checks and Diagnostics**

```csharp
public class StreamProcessorHealthCheck : IHealthCheck
{
    private readonly Channel<HealthData> _first_channel = Channel.CreateUnbounded<HealthData>();
    private readonly Channel<HealthData> _second_channel = Channel.CreateUnbounded<HealthData>();

    public async Task<HealthCheckResult> CheckHealthAsync(HealthCheckContext context, CancellationToken cancellationToken = default)
    {
        try
        {
            // Build a unified async enumerable from both channels
            var merger = new AsyncEnumerable<HealthData>()
                .Unify(_first_channel.Reader.ReadAllAsync(),  "Source1")
                .Unify(_second_channel.Reader.ReadAllAsync(), "Source2");

            // Test the processing pipeline with two items
            var testData1 = new HealthData { Timestamp = DateTime.UtcNow, Status = "Test1" };
            _first_channel.Writer.TryWrite(testData1);

            var testData2 = new HealthData { Timestamp = DateTime.UtcNow, Status = "Test2" };
            _second_channel.Writer.TryWrite(testData2);

            var processed = await merger
                .Take(1)
                .Cases(data => data.Status.StartsWith("Test", StringComparison.OrdinalIgnoreCase))
                .SelectCase(test => $"Health check passed at {test.Timestamp:O}")
                .AllCases()
                .FirstOrDefaultAsync(cancellationToken);

            return processed != null
                ? HealthCheckResult.Healthy("Stream processing pipeline is operational")
                : HealthCheckResult.Degraded("Stream processing pipeline may be slow");
        }
        catch (Exception ex)
        {
            return HealthCheckResult.Unhealthy("Stream processing pipeline failed", ex);
        }
    }
}
```

### 10.6 Testing Strategies

#### 1. **Unit Testing Processing Logic**

```csharp
[Test]
public async Task TestOrderProcessingLogic()
{
    // Arrange
    var testOrders = new[]
    {
        new Order { Id = 1, Amount = 1500, IsInternational = false, Customer = new Customer { IsVIP = false } },
        new Order { Id = 2, Amount = 500, IsInternational = true, Customer = new Customer { IsVIP = false } },
        new Order { Id = 3, Amount = 800, IsInternational = false, Customer = new Customer { IsVIP = true } }
    }.ToAsyncEnumerable();
  
    // Act
    var results = await testOrders
        .Cases(
            order => order.Amount > 1000,
            order => order.IsInternational,
            order => order.Customer.IsVIP
        )
        .SelectCase(
            highValue => new ProcessedOrder { Id = highValue.Id, Category = "HighValue" },
            international => new ProcessedOrder { Id = international.Id, Category = "International" },
            vip => new ProcessedOrder { Id = vip.Id, Category = "VIP" },
            standard => new ProcessedOrder { Id = standard.Id, Category = "Standard" }
        )
        .AllCases()
        .ToListAsync();
  
    // Assert
    Assert.AreEqual(3, results.Count);
    Assert.AreEqual("HighValue", results.First(r => r.Id == 1).Category);
    Assert.AreEqual("International", results.First(r => r.Id == 2).Category);
    Assert.AreEqual("VIP", results.First(r => r.Id == 3).Category);
}
```

#### 2. **Integration Testing with Mock Streams**

```csharp
[Test]
public async Task TestStreamProcessingIntegration()
{
    // Arrange
    var ch = Channel.CreateUnbounded<LogEntry>();
    var merger = new AsyncEnumerable<LogEntry>()
        .Unify(ch.Reader.ReadAllAsync(), "mock");
    var processedLogs = new List<string>();
  
    // Set up processing pipeline
    var processingTask = merger
        .Cases(
            log => log.Level == "ERROR",
            log => log.Level == "WARNING"
        )
        .SelectCase(
            error => $"ALERT: {error.Message}",
            warning => $"WARN: {warning.Message}",
            info => $"INFO: {info.Message}"
        )
        .ForEachCase(
            alert => processedLogs.Add(alert),
            warn => processedLogs.Add(warn),
            info => processedLogs.Add(info)
        )
        .AllCases()
        .Take(3)  // Only process first 3 items for test
        .ToListAsync();
  
    // Act - Publish test data
    ch.Writer.TryWrite(new LogEntry { Level = "ERROR", Message = "Critical error" });
    ch.Writer.TryWrite(new LogEntry { Level = "WARNING", Message = "Warning message" });
    ch.Writer.TryWrite(new LogEntry { Level = "INFO", Message = "Info message" });
  
    var results = await processingTask;
  
    // Assert
    Assert.AreEqual(3, results.Count);
    Assert.IsTrue(processedLogs.Any(log => log.Contains("ALERT: Critical error")));
    Assert.IsTrue(processedLogs.Any(log => log.Contains("WARN: Warning message")));
    Assert.IsTrue(processedLogs.Any(log => log.Contains("INFO: Info message")));
}
```

#### 3. **Performance Testing**

```csharp
[Test]
public async Task TestStreamProcessingPerformance()
{
    // Arrange
    const int itemCount = 100000;
    var testData = Enumerable.Range(1, itemCount)
        .Select(i => new DataItem { Id = i, Value = i % 10 })
        .ToAsyncEnumerable();
  
    var stopwatch = Stopwatch.StartNew();
  
    // Act
    var results = await testData
        .Cases(
            item => item.Value < 3,
            item => item.Value < 7
        )
        .SelectCase(
            low => new ProcessedItem { Id = low.Id, Category = "Low" },
            medium => new ProcessedItem { Id = medium.Id, Category = "Medium" },
            high => new ProcessedItem { Id = high.Id, Category = "High" }
        )
        .AllCases()
        .ToListAsync();
  
    stopwatch.Stop();
  
    // Assert
    Assert.AreEqual(itemCount, results.Count);
  
    var itemsPerSecond = itemCount / stopwatch.Elapsed.TotalSeconds;
    Assert.IsTrue(itemsPerSecond > 10000, $"Processing rate too slow: {itemsPerSecond:F2} items/second");
  
    Console.WriteLine($"Processed {itemCount} items in {stopwatch.Elapsed}");
    Console.WriteLine($"Rate: {itemsPerSecond:F2} items/second");
}
```

---

## 11. What's Next

More More connectors !! Even a ** SPARK ** abstraction layer !!

---

*For detailed API documentation and layer-specific guides, refer to:*

- *[API Reference](API-Reference.md) - Complete method signatures and usage examples*
- *[DataFlow.Data Layer](DataFlow-Data-Layer.md) - Data access and transformation utilities*
- *[DataFlow.Extensions Layer](DataFlow-Extensions-Layer.md) - Extension methods and processing patterns*
- *[DataFlow.Framework Layer](DataFlow-Framework-Layer.md) - Core framework components and streaming infrastructure*
